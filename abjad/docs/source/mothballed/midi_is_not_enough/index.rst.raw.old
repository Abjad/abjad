Why MIDI is not enough
======================

Given that Abjad models written musical score, it might seem odd for MIDI to be
even mentioned in this manual. Yet, until fairly recently, MIDI has played a
role (sometimes tangential, other times fundamental) in a variety of software
tools related to music notation and engraving.


A very brief overview of MIDI
-----------------------------

MIDI (Musical Instrument Digital Interface) was first introduced in 1981 by
Dave Smith, the founder of Sequential Circuits. The original purpose of MIDI
was to allow the communication between different electronic musical
instruments; more specifically, to allow one device to send **control** data to
another device. Typical messages might be "note On" (play a *note*) "note Off"
(turn off a *note*). A MIDI "note" message, for example, is composed of three
bytes: the first byte (the Status byte) tells the device what kind of message
this is (e.g. a Note On message). The second byte encodes key number (which key
was pressed) and the third byte, velocity (how hard the key was pressed). It
should be clear that a *Note* in this context means something very different
than *Note* in the context of a traditional printed score. While the bias
towards keyboard interfaces is clear in the definition of the MIDI Note control
message, one can still give the MIDI note a more general use by reinterpreting
"key number" as pitch and "velocity" as loudness, the usual perceptual
correlates of these control changes as well as the most meaningful musical
parameters in western music.

With the subsequent proliferation of music production software, the SMF
(Standard Midi File) was introduced to allow the recording and storage of the
control data from a MIDI stream. The SMF required a time stamp to keep track of
when control messages took place. These are called "delta-times" in the SMF
specification.

*"The MTrk chunk type is where actual song data is stored. It is simply a
stream of MIDI events (and non-MIDI events), preceded by delta-time values."*

In combination with the MIDI Note message, the addition of duration now allowed
one to have a minimal but sufficient **machine** representation--a machine
score--of music requiring only these parameters: duration, pitch and loudness.
Such is the case of most piano music.


Limitations of MIDI from the point of view of score modeling
------------------------------------------------------------

But, alas, there is much more information in a printed score that can not be
practically encoded in a SMF. Common musical notions such as meter, clef, key
signature, articulation, to name only a few, are ignored. A desire to include
some of these concepts in MIDI is evident in the inclusion of some so called
*meta-events*. From the SMF specification: " specifies non-MIDI information
useful to this format or to sequencers." Examples of *meta-events* are *Time
Signature* and *Key Signature*. In addition to the semantic elements just
mentioned, there are also the typographical elements (such as line thickness,
spacing, color, fonts, etc.) that all printed scores carry. This extra layer of
information is completely absent in a SMF. However, from the point of view of
encoding a printed score, the main limitation of MIDI is not the lack musical
features or the absence of typographical data, but the assumption that musical
durations, pitches and loudnesses can be each fully and efficiently encoded
with integers or even fractions. In a printed score, this is not the case for
any of them. MIDI encodes only *magnitudes*: time interval magnitudes, pitch
interval magnitudes, velocity magnitudes. While these may be sufficient
attributes for an automated piano performance, they are not all the attributes
of notes in a printed score.


Written note durations vs. MIDI delta-times
-------------------------------------------

Assume a fixed tempo has been set. Assume that all magnitudes are represented
with (and limited to) rational numbers. A time interval magnitude d = 1/4 has
an infinity of equivalent representations in terms of magnitude: d = 1/4 = 1/8
* 2 = 1/8 + 1/16 * 2 ... etc. So, for example, while equivalent in magnitude,
* these are not the same notated durations:

<abjad>
m1 = Measure((1, 4), [Note("c'4")])
m2 = Measure((1, 4), 2 * Note(0, (1, 8)))
spannertools.Tie(m2)
m3 = Measure((1, 4), [Note(0, (1, 8))] + 2 * Note(0, (1, 16)))
spannertools.Tie(m3)
r = scoretools.RhythmicStaff([m1, m2, m3])
</abjad>

<abjad>
show(r)
</abjad>


Written note pitch vs. MIDI note-on
-----------------------------------

A similar thing happens with pitches. In MIDI, key (pitch) number 61 is a half
tone above middle C. But how is this pitch to be notated? As a C sharp or a B
flat?

<abjad>
m1 = Measure((1, 4), [Note(1, (1, 4))])
m2 = Measure((1, 4), [Note(('df', 4), (1, 4))])
r = Staff([m1, m2])
</abjad>

<abjad>
show(r)
</abjad>


Conclusion
----------

MIDI was not designed for score representation. MIDI is a simple communication
protocol intended for real-time control. As such, it naturally lacks the
adequate model to represent the full range of information found in printed
scores.
